{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9d147b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cosyvoice.utils.file_utils import load_wav\n",
    "import torchaudio\n",
    "import torch\n",
    "import os\n",
    "from hyperpyyaml import load_hyperpyyaml\n",
    "from tqdm import tqdm\n",
    "import uuid\n",
    "import threading\n",
    "import soundfile as sf\n",
    "\n",
    "from cosyvoice.cli.frontend import CosyVoiceFrontEnd\n",
    "from cosyvoice.cli.model import CosyVoiceModel\n",
    "from cosyvoice.cli.cosyvoice import CosyVoice\n",
    "\n",
    "class CosyVoiceFrontEnd_eval(CosyVoiceFrontEnd):\n",
    "    def frontend_zero_shot(self, tts_text, prompt_text, prompt_speech_16k, resample_rate):\n",
    "        tts_text_token, tts_text_token_len = self._extract_text_token(tts_text)\n",
    "        prompt_text_token, prompt_text_token_len = self._extract_text_token(prompt_text)\n",
    "        prompt_speech_resample = torchaudio.transforms.Resample(orig_freq=16000, new_freq=resample_rate)(prompt_speech_16k)\n",
    "        speech_feat, speech_feat_len = self._extract_speech_feat(prompt_speech_resample)\n",
    "        speech_token, speech_token_len = self._extract_speech_token(prompt_speech_16k)\n",
    "        if resample_rate == 24000:\n",
    "            # cosyvoice2, force speech_feat % speech_token = 2\n",
    "            token_len = min(int(speech_feat.shape[1] / 2), speech_token.shape[1])\n",
    "            speech_feat, speech_feat_len[:] = speech_feat[:, :2 * token_len], 2 * token_len\n",
    "            speech_token, speech_token_len[:] = speech_token[:, :token_len], token_len\n",
    "        embedding = self._extract_spk_embedding(prompt_speech_16k)\n",
    "        model_input = {'text': tts_text_token, 'text_len': tts_text_token_len,\n",
    "                       'prompt_text': prompt_text_token, 'prompt_text_len': prompt_text_token_len,\n",
    "                       'llm_prompt_speech_token': speech_token, 'llm_prompt_speech_token_len': speech_token_len,\n",
    "                       'flow_prompt_speech_token': speech_token, 'flow_prompt_speech_token_len': speech_token_len,\n",
    "                       'prompt_speech_feat': speech_feat, 'prompt_speech_feat_len': speech_feat_len,\n",
    "                       'llm_embedding': embedding, 'flow_embedding': embedding}\n",
    "        return model_input\n",
    "\n",
    "class CosyVoiceModel_eval(CosyVoiceModel):\n",
    "    def load(self, llm_model, flow_model, hift_model):\n",
    "        self.llm.load_state_dict(torch.load(llm_model, map_location=self.device), strict=True)\n",
    "        self.llm.to(self.device).eval()\n",
    "        if self.fp16 is True:\n",
    "            self.llm.half()\n",
    "        self.flow.load_state_dict(torch.load(flow_model, map_location=self.device), strict=False)\n",
    "        self.flow.to(self.device).eval()\n",
    "        # in case hift_model is a hifigan model\n",
    "        hift_state_dict = {k.replace('generator.', ''): v for k, v in torch.load(hift_model, map_location=self.device).items()}\n",
    "        self.hift.load_state_dict(hift_state_dict, strict=True)\n",
    "        self.hift.to(self.device).eval()\n",
    "\n",
    "    def token2wav(self, token, prompt_token, prompt_feat, embedding, uuid, finalize=False, speed=1.0):\n",
    "        tts_mel, flow_cache = self.flow.inference(token=token.to(self.device),\n",
    "                                                  token_len=torch.tensor([token.shape[1]], dtype=torch.int32).to(self.device),\n",
    "                                                  prompt_token=prompt_token.to(self.device),\n",
    "                                                  prompt_token_len=torch.tensor([prompt_token.shape[1]], dtype=torch.int32).to(self.device),\n",
    "                                                  prompt_feat=prompt_feat.to(self.device),\n",
    "                                                  prompt_feat_len=torch.tensor([prompt_feat.shape[1]], dtype=torch.int32).to(self.device),\n",
    "                                                  embedding=embedding.to(self.device),\n",
    "                                                  flow_cache=self.flow_cache_dict[uuid])\n",
    "        self.flow_cache_dict[uuid] = flow_cache\n",
    "\n",
    "        # mel overlap fade in out\n",
    "        if self.mel_overlap_dict[uuid].shape[2] != 0:\n",
    "            tts_mel = fade_in_out(tts_mel, self.mel_overlap_dict[uuid], self.mel_window)\n",
    "        # append hift cache\n",
    "        if self.hift_cache_dict[uuid] is not None:\n",
    "            hift_cache_mel, hift_cache_source = self.hift_cache_dict[uuid]['mel'], self.hift_cache_dict[uuid]['source']\n",
    "            tts_mel = torch.concat([hift_cache_mel, tts_mel], dim=2)\n",
    "        else:\n",
    "            hift_cache_source = torch.zeros(1, 1, 0)\n",
    "        # keep overlap mel and hift cache\n",
    "        if finalize is False:\n",
    "            self.mel_overlap_dict[uuid] = tts_mel[:, :, -self.mel_overlap_len:]\n",
    "            tts_mel = tts_mel[:, :, :-self.mel_overlap_len]\n",
    "            tts_speech, tts_source = self.hift.inference(speech_feat=tts_mel, cache_source=hift_cache_source)\n",
    "            if self.hift_cache_dict[uuid] is not None:\n",
    "                tts_speech = fade_in_out(tts_speech, self.hift_cache_dict[uuid]['speech'], self.speech_window)\n",
    "            self.hift_cache_dict[uuid] = {'mel': tts_mel[:, :, -self.mel_cache_len:],\n",
    "                                          'source': tts_source[:, :, -self.source_cache_len:],\n",
    "                                          'speech': tts_speech[:, -self.source_cache_len:]}\n",
    "            tts_speech = tts_speech[:, :-self.source_cache_len]\n",
    "        else:\n",
    "            if speed != 1.0:\n",
    "                assert self.hift_cache_dict[uuid] is None, 'speed change only support non-stream inference mode'\n",
    "                tts_mel = F.interpolate(tts_mel, size=int(tts_mel.shape[2] / speed), mode='linear')\n",
    "            tts_speech, tts_source = self.hift.inference(speech_feat=tts_mel, cache_source=hift_cache_source)\n",
    "            if self.hift_cache_dict[uuid] is not None:\n",
    "                tts_speech = fade_in_out(tts_speech, self.hift_cache_dict[uuid]['speech'], self.speech_window)\n",
    "        return tts_speech\n",
    "\n",
    "    def tts(self, text, flow_embedding, llm_embedding=torch.zeros(0, 192),\n",
    "            prompt_text=torch.zeros(1, 0, dtype=torch.int32),\n",
    "            llm_prompt_speech_token=torch.zeros(1, 0, dtype=torch.int32),\n",
    "            flow_prompt_speech_token=torch.zeros(1, 0, dtype=torch.int32),\n",
    "            prompt_speech_feat=torch.zeros(1, 0, 80), stream=False, speed=1.0, **kwargs):\n",
    "        # this_uuid is used to track variables related to this inference thread\n",
    "        this_uuid = str(uuid.uuid1())\n",
    "        with self.lock:\n",
    "            self.tts_speech_token_dict[this_uuid], self.llm_end_dict[this_uuid] = [], False\n",
    "            self.hift_cache_dict[this_uuid] = None\n",
    "            self.mel_overlap_dict[this_uuid] = torch.zeros(1, 80, 0)\n",
    "            self.flow_cache_dict[this_uuid] = torch.zeros(1, 80, 0, 2)\n",
    "        p = threading.Thread(target=self.llm_job, args=(text, prompt_text, llm_prompt_speech_token, llm_embedding, this_uuid))\n",
    "        p.start()\n",
    "        # deal with all tokens\n",
    "        p.join()\n",
    "        this_tts_speech_token = torch.tensor(self.tts_speech_token_dict[this_uuid]).unsqueeze(dim=0)\n",
    "        this_tts_speech = self.token2wav(token=this_tts_speech_token,\n",
    "                                            prompt_token=flow_prompt_speech_token,\n",
    "                                            prompt_feat=prompt_speech_feat,\n",
    "                                            embedding=flow_embedding,\n",
    "                                            uuid=this_uuid,\n",
    "                                            finalize=True,\n",
    "                                            speed=speed)\n",
    "        return this_tts_speech\n",
    "    \n",
    "    def inference(self, flow_embedding, \n",
    "            llm_prompt_speech_token,\n",
    "            flow_prompt_speech_token,\n",
    "            prompt_speech_feat,\n",
    "            stream=False, speed=1.0, \n",
    "            n_timesteps=10, temperature=1.0, alpha=1.0, solver='euler', **kwargs):\n",
    "\n",
    "        prompt_token = flow_prompt_speech_token\n",
    "        prompt_feat = prompt_speech_feat\n",
    "        embedding = flow_embedding\n",
    "\n",
    "        tts_mel, tp, sigma = self.flow.inference(token=llm_prompt_speech_token.to(self.device),\n",
    "                                                  token_len=torch.tensor([llm_prompt_speech_token.shape[1]], dtype=torch.int32).to(self.device),\n",
    "                                                  prompt_token=prompt_token.to(self.device),\n",
    "                                                  prompt_token_len=torch.tensor([prompt_token.shape[1]], dtype=torch.int32).to(self.device),\n",
    "                                                  prompt_feat=prompt_feat.to(self.device),\n",
    "                                                  prompt_feat_len=torch.tensor([prompt_feat.shape[1]], dtype=torch.int32).to(self.device),\n",
    "                                                  embedding=embedding.to(self.device),\n",
    "                                                  n_timesteps=n_timesteps,\n",
    "                                                  temperature=temperature,\n",
    "                                                  alpha=alpha,\n",
    "                                                  solver=solver)\n",
    "\n",
    "        hift_cache_source = torch.zeros(1, 1, 0)\n",
    "        tts_speech, tts_source = self.hift.inference(speech_feat=tts_mel, cache_source=hift_cache_source)\n",
    "        return tts_speech, tp, sigma\n",
    "\n",
    "\n",
    "class CosyVoice_eval(CosyVoice):\n",
    "    def __init__(self, model_dir, flow_dir, config_dir, load_jit=False, load_trt=False, fp16=False):\n",
    "        #super().__init__(model_dir, load_jit, load_trt, fp16)\n",
    "        self.instruct = True if '-Instruct' in model_dir else False\n",
    "        self.model_dir = model_dir\n",
    "        self.fp16 = fp16\n",
    "        if not os.path.exists(model_dir):\n",
    "            model_dir = snapshot_download(model_dir)\n",
    "        with open(config_dir, 'r') as f:\n",
    "            configs = load_hyperpyyaml(f)\n",
    "        self.frontend = CosyVoiceFrontEnd(configs['get_tokenizer'],\n",
    "                                          configs['feat_extractor'],\n",
    "                                          '{}/campplus.onnx'.format(model_dir),\n",
    "                                          '{}/speech_tokenizer_v1.onnx'.format(model_dir),\n",
    "                                          '{}/spk2info.pt'.format(model_dir),\n",
    "                                          configs['allowed_special'])\n",
    "        self.sample_rate = configs['sample_rate']\n",
    "        if torch.cuda.is_available() is False and (load_jit is True or load_trt is True or fp16 is True):\n",
    "            load_jit, load_trt, fp16 = False, False, False\n",
    "            logging.warning('no cuda device, set load_jit/load_trt/fp16 to False')\n",
    "        self.model = CosyVoiceModel_eval(configs['llm'], configs['flow'], configs['hift'], fp16)\n",
    "        self.model.load('{}/llm.pt'.format(model_dir),\n",
    "                        flow_dir,\n",
    "                        '{}/hift.pt'.format(model_dir))\n",
    "        if load_jit:\n",
    "            self.model.load_jit('{}/llm.text_encoder.{}.zip'.format(model_dir, 'fp16' if self.fp16 is True else 'fp32'),\n",
    "                                '{}/llm.llm.{}.zip'.format(model_dir, 'fp16' if self.fp16 is True else 'fp32'),\n",
    "                                '{}/flow.encoder.{}.zip'.format(model_dir, 'fp16' if self.fp16 is True else 'fp32'))\n",
    "        if load_trt:\n",
    "            self.model.load_trt('{}/flow.decoder.estimator.{}.mygpu.plan'.format(model_dir, 'fp16' if self.fp16 is True else 'fp32'),\n",
    "                                '{}/flow.decoder.estimator.fp32.onnx'.format(model_dir),\n",
    "                                self.fp16)\n",
    "        del configs\n",
    "    \n",
    "    def inference_zero_shot(self, prompt_text, target_text, prompt_token, target_token, prompt_speech_16k, prompt_embed, stream=False, speed=1.0, text_frontend=True, n_timesteps=10, temperature=1.0, alpha=1.0, solver='euler'):\n",
    "        prompt_speech_resample = torchaudio.transforms.Resample(orig_freq=16000, new_freq=self.sample_rate)(prompt_speech_16k)\n",
    "        prompt_feat, prompt_feat_len = self.frontend._extract_speech_feat(prompt_speech_resample)\n",
    "\n",
    "        # The following commented-out codes are about using LLM to generate tokens\n",
    "        '''\n",
    "        target_text_token, target_text_token_len = self.frontend._extract_text_token(target_text)\n",
    "        prompt_text_token, prompt_text_token_len = self.frontend._extract_text_token(prompt_text)\n",
    "        \n",
    "        target_token = []\n",
    "        for i in self.model.llm.inference(text=target_text_token.to(self.model.device),\n",
    "                                text_len=torch.tensor([target_text_token.shape[1]], dtype=torch.int32).to(self.model.device),\n",
    "                                prompt_text=prompt_text_token.to(self.model.device),\n",
    "                                prompt_text_len=torch.tensor([prompt_text_token.shape[1]], dtype=torch.int32).to(self.model.device),\n",
    "                                prompt_speech_token=prompt_token.to(self.model.device),\n",
    "                                prompt_speech_token_len=torch.tensor([prompt_token.shape[1]], dtype=torch.int32).to(self.model.device),\n",
    "                                embedding=prompt_embed.to(self.model.device)):\n",
    "            target_token.append(i)\n",
    "        target_token = torch.tensor(target_token).unsqueeze(0).to(self.model.device)\n",
    "        '''\n",
    "        model_input = {'llm_prompt_speech_token': target_token,\n",
    "                       'flow_prompt_speech_token': prompt_token,\n",
    "                       'prompt_speech_feat': prompt_feat, 'prompt_speech_feat_len': prompt_feat_len,\n",
    "                       'flow_embedding': prompt_embed.to(self.model.device)}\n",
    "        model_output, tp, sigma = self.model.inference(**model_input, stream=stream, speed=speed, n_timesteps=n_timesteps, temperature=temperature, alpha=alpha, solver=solver)\n",
    "        return model_output, tp, sigma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3687c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosyvoice = CosyVoice_eval('pretrained_models/CosyVoice-300M', \n",
    "                           'pretrained_models/CosyVoice-sfm-epoch_199_step_200201.pt',\n",
    "                           \"configs/cosyvoice.yaml\",\n",
    "                           load_jit=False, load_trt=False, fp16=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75b39c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "wavs_dict = {}\n",
    "with open(\"CosyVoice-libritts-data/test-clean/wav.scp\", \"r\") as f:\n",
    "    wavs = f.readlines()\n",
    "for wav in wavs:\n",
    "    wavs_dict[wav.split(\" \")[0]] = wav.split(\" \", 1)[1].strip(\"\\n\").replace(\"xxx\", \"your LibriTTS wav path\")\n",
    "\n",
    "# texts_dict = {}\n",
    "# with open(\"CosyVoice-libritts-data/test-clean/text\", \"r\") as f:\n",
    "#     texts = f.readlines()\n",
    "# for t in texts:\n",
    "#     texts_dict[t.split(\" \")[0]] = t.split(\" \", 1)[1].strip(\"\\n\")\n",
    "\n",
    "tokens = torch.load(\"CosyVoice-libritts-data/test-clean/utt2speech_token.pt\")\n",
    "embeds = torch.load(\"CosyVoice-libritts-data/test-clean/utt2embedding.pt\")\n",
    "\n",
    "pairs_dict = {}\n",
    "with open(\"../libritts-cross_sentence-infer/test_pairs.txt\", \"r\") as f:\n",
    "    pairs = f.readlines()\n",
    "for pair in pairs:\n",
    "    prompt, target = pair.strip(\"\\n\").split(\" \")\n",
    "    pairs_dict[target] = prompt\n",
    "\n",
    "token_dict = torch.load(\"../libritts-cross_sentence-infer/test_target_tokens.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9d6ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder = \"xxx\"\n",
    "# import os\n",
    "# if os.path.exists(folder):\n",
    "#     os.system(f\"rm -r {folder}\")\n",
    "# os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "alpha = 2.0\n",
    "n_timesteps = 10\n",
    "solver = 'euler'\n",
    "\n",
    "tps = 0.0\n",
    "sigma_ps = 0.0\n",
    "\n",
    "SEED = 1234\n",
    "import random\n",
    "random.seed(SEED)\n",
    "import numpy as np\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "for target in tqdm(pairs_dict.keys()):\n",
    "    prompt = pairs_dict[target]\n",
    "    prompt_speech_16k = load_wav(wavs_dict[prompt], 16000)\n",
    "    prompt_text = None #texts_dict[prompt]\n",
    "    target_text = None #texts_dict[target]\n",
    "\n",
    "    prompt_token = torch.tensor(tokens[prompt]).unsqueeze(0)\n",
    "    prompt_embed = torch.tensor(embeds[prompt]).unsqueeze(0)\n",
    "    target_token = torch.tensor(token_dict[target]).unsqueeze(0)\n",
    "\n",
    "    output, tp, sigma_p = cosyvoice.inference_zero_shot(prompt_text, target_text, prompt_token, target_token, prompt_speech_16k, prompt_embed, n_timesteps=n_timesteps, alpha=alpha, solver=solver)\n",
    "    output = output.cpu().squeeze(0)\n",
    "    break\n",
    "    # tps += tp \n",
    "    # sigma_ps += sigma_p\n",
    "#     sf.write(f'{folder}/{target+\".wav\"}', output, 22050, 'PCM_24')\n",
    "\n",
    "# tps = round(tps/len(pairs_dict.keys()), 8)\n",
    "# sigma_ps = round(sigma_ps/len(pairs_dict.keys()), 8)\n",
    "# print(tps, sigma_ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b8b5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "ipd.display(ipd.Audio(output, rate=22050))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cosyvoice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
